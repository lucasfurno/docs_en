---
layout: page-documentation-md
title: Data Streaming
description: Data Streaming is an Edge Analytics product that allows you to feed your SIEM, big data and stream processing platforms with access to your content and applications data, in real time, adding even more intelligence to your business.
meta_tags: data, streaming,  edge computing
namespace: documentation_products_data_streaming
permalink: "/documentation/products/data-streaming/"
permalink_pt-br: "/documentacao/produtos/data-streaming/"
---
# Data **Streaming**

[Edit on GitHub <svg width="14" height="14" xmlns="http://www.w3.org/2000/svg"><g fill="none" stroke="#F3652B"><path d="M4.81.71H.672v11.43H12.1V8.001" stroke-width=".8"/><path d="M6.87.786h5.155V5.94M6.31 6.5L12.026.786"/></g></svg>](https://github.com/aziontech/docs_en/edit/master/products/data-streaming/2021-01-14-index.md)

Data Streaming is an Edge Analytics product that allows you to feed your SIEM, big data and stream processing platforms with access to your content and applications data, in real time, adding even more intelligence to your business.

This integration allows you to analyze the behavior of your users and the performance of your content, applications and troubleshooting, in a simple and agile way.

In addition, Data Streaming follows the good practices of information traffic by using ASCII encoding to avoid parser issues, that is, problems in data interpretation. 

  > 1. [Accessing Data Streaming](#Accessingdatastreaming)
  >
  > 2. [Selecting Data Sources](#Selectingdatasources)
  >
  > 3. [Using a Template](#Usingatemplate)
  >
  > 4. [Associating Domains](#Associatingdomains)
  >
  > 5. [Setting the Endpoint](#Settingtheendpoint)
  >
  >    *[Apache Kafka](#ApacheKafka)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *[AWS Kinesis Data Firehose](#AWS)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *[Data Dog](#DataDog)*
  >
  >    *[Elasticsearch](#Elasticsearch)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *[Google BigQuery](#Google)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *[IBM QRadar](#IBM)*          
  >
  >    *[Simple Storage Service](S3)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *[Splunk](#Splunk)*&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*[Standard HTTP/HTTPS POST](#StandardHTTPHTTPSPOST)*
  >
  > 6. [Customizing the Payload](#Customizingthepayload)
  >
  > 7. [Activating your Settings](#Activatingyoursettings)

---

## 1. Accessing Data Streaming {#Accessingdatastreaming}

Enter [Real-Time Manager](https://manager.azion.com/). Click on **Data Streaming** either on the Edge Analytics or the Products menu, which is on the upper left. 

Configure your Data Streaming settings according to the following possibilities. 

> Fields marked with an asterisk are required. 

---

## 2. Selecting Data Sources {#Selectingdatasources}

The first step is choosing the Data Source, which represents the application at Azion that generated the event logs. In doing so, you must select where your data will be collected from. You have the following options:

### **Edge Applications**

It displays the data from requests made to your Edge Applications at Azion.

| Variables                        | Description                                                  |
| -------------------------------- | ------------------------------------------------------------ |
| $bytes_sent                      | Bytes sent to the user, including header and body.           |
| $client                          | Unique Azion customer identifier.                            |
| $configuration                   | Unique Azion configuration identifier.                       |
| $country                         | Country name of the remote client, for example “Russian Federation”, “United States”. Geolocation detection by IP address. |
| $host                            | Host information sent on the request line; or HTTP header Host field. |
| $http_referrer                   | Information from the last page the user was on before making the request. |
| $http_user_agent                 | The identification of the application that made the request, for example: Mozilla/5.0 (Windows NT 10.0; Win64; x64). |
| $proxy_status                    | HTTP Status Code of origin (”-” in case of cache).           |
| $remote_addr                     | IP address of the request.                                   |
| $remote_port                     | Remote port of the request.                                  |
| $request_length                  | Request size, including request line, headers and body.      |
| $request_method                  | Request method; usually “GET” or “POST”.                     |
| $request_time                    | Request processing time with resolution in milliseconds.     |
| $request_uri                     | URI of the request made by the user, without the Host and Protocol information. |
| $requestPath                     | The request URI without Query String, Host and Protocol information. |
| $requestQuery                    | Only the URI parameters of the request.                      |
| $scheme                          | Request scheme “http” or “https".                            |
| $sent_http_content_type          | “Content-Type” header sent in the origin’s response.         |
| $sent_http_x_original_image_size | “X-Original-Image-Size” header sent in the origin’s response (used by IMS to inform original image size). |
| $server_protocol                 | The protocol of the connection established, usually “HTTP/1.1” or “HTTP/2.0”. |
| $ssl_cipher                      | Cipher string used to establish SSL connection.              |
| $ssl_protocol                    | The protocol for an established SSL connection, for example “TLS v1.2”. |
| *$state                          | Name of the remote client’s state, for example: “RS”, “SP”. <br/>Geolocation detection of IP address. |
| $status                          | The status code of the request, for example: 200.            |
| $tcpinfo_rtt                     | The RTT time in microseconds measured by Edge for the user.  |
| $time                            | Request date and time.                                       |
| $upstream_bytes_received         | Number of bytes received by the origin’s Edge, if the content is not cached. |
| $upstream_cache_status           | Status of the Edge cache. It can assume the values “MISS”, “BYPASS”, “EXPIRED”, “STALE”, “UPDATING”, “REVALIDATED” or “HIT”. |
| $upstream_connect_time           | Time in milliseconds for Edge to establish a connection with the origin (“0” in case of KeepAlive and “-“ in case of cache). |
| $upstream_header_time            | Time in milliseconds for Edge to receive the origin’s response headers ( “-“ in case of cache). |
| $upstream_response_time          | Time in milliseconds for Edge to receive all of the response from the origin, including headers and body (“-“ in case of cache). |
| $upstream_status                 | HTTP Status Code of the origin (“-“ in case of cache).       |
| $waf_attack_action               | It reports WAF’s action regarding the action ($BLOCK, $PASS, $LEARNING_BLOCK, $LEARNING_PASS). |
| $waf_attack_family               | It informs the classification of the WAF infraction detected in the request (SQL, XSS, TRAVERSAL, among others). |

---

The variables ``$upstream_bytes_received``; ``$upstream_cache_status``; ``$upstream_connect_time``; ``$upstream_header_time``; ``$upstream_response_time`` and ``$upstream_status`` can have more than one comma-separated element. When a connection is triggered (either by internal redirection or choice of source with Load Balancer, for example), each value contained in the field represents the respective initiated connection. The field can be separated by a comma (multiple IPs) or a colon (internal redirection). 

If several servers are contacted during the request phase, the addresses are separated by commas, for example: *192.168.1.1:80***,** *192.168.1.2:80***,** *unix:/tmp/sock*. If several servers were contacted during the request processing, their addresses are separated by commas, for instance, *192.168.1.1:80, 192.168.1.2:80*. If an internal redirect from one server group to another happens, initiated by *X-Accel-Redirect* or *Error Responses*, then the server addresses from different groups are separated by colons. For example, *192.168.1.1:80, 192.168.1.2:80, unix:/tmp/sock **:** 192.168.10.1:80, 192.168.10.2:80*. If a server cannot be selected, the variable keeps the name of the server group.
Considering multiple values as transitions in the connection, the last value tends to be the most important. If you use the *Error Responses* feature, you'll see two values on upstream fields that represent the status of the origin and the result of the request that was made to get the content to be delivered instead. So, in normal cases, you may get **502 : 200.**

The *502* is the HTTP error code for the response of the first try to get content from the origin server. Because it returned an error *502*, considering you have configured Error Responses for status 502, another request will be made in order to get the URI defined. Then, the page will be delivered and the HTTP status added to the *upstream* fields, respecting its position for all of them, in this example, making the *502 : 200* composition.


### **Edge Functions**

In case you have contracted [Edge Functions](https://www.azion.com/pt-br/documentacao/produtos/edge-functions/), you may use the following available variables for the defined data source.

| Variable          | Description                                                  |
| ----------------- | ------------------------------------------------------------ |
| $time             | Request date and time.                                       |
| $global_id        | Settings ID.                                                 |
| $edge_function_id | Edge Function ID.                                            |
| $request_id       | Request ID.                                                  |
| $log_level        | Level of the log generator (ERROR, WARN, INFO, DEBUG, TRACE). |
| $log_message      | Message used in the log function.                            |



### **WAF**

If you have contracted the [Web Application Firewall](https://www.azion.com/pt-br/docs/produtos/web-application-firewall/) product, the WAF Events data source will display the requests analyzed by WAF to allow you to map the score assigned to the request, the WAF rules that matched, the reason for the block and more.

| Variable           | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| $blocked           | It informs whether the WAF blocked the action or not; 0 when not blocked and 1 when blocked. When in “Learning Mode”, it will not be blocked, regardless of the return. |
| $client            | Unique Azion customer identifier.                            |
| $configuration     | Unique Azion configuration identifier.                       |
| $country           | Country name of the remote client, for example “Russian Federation”, “United States”. Geolocation detection by IP address. |
| $headers           | Request headers analyzed by WAF.                             |
| $host              | Host information sent on the request line; or Host field of the HTTP header. |
| $remote_addr       | IP address of the request.                                   |
| $requestPath       | The request URI without Query String, Host and Protocol information. |
| $requestQuery      | Only the URI parameters of the request.                      |
| $server_protocol   | The protocol of the connection established, usually “HTTP/1.1” or “HTTP/2.0”. |
| $time              | Timestamp of the start of the request.                       |
| $version           | The version of Azion Log used.                               |
| $waf_args          | The request arguments.                                       |
| $waf_attack_action | It reports WAF’s action regarding the action ($BLOCK, $PASS, $LEARNING_BLOCK, $LEARNING_PASS). |
| $waf_attack_family | It informs the classification of the WAF infraction detected in the request (SQL, XSS, TRAVERSAL, among others) |
| $waf_learning      | It informs if WAF is in learning mode, usually 0 or 1.       |
| $waf_match         | List of infractions found in the request, it is formed by key-value elements; the key refers to the type of violation detected; the value shows the string that generated the infraction. |
| $waf_score         | It reports the score that will be increased in case of match. |
| $waf_server        | Hostname used in the request.                                |
| $waf_uri           | URI used in the request.                                     |

---

## 3. Using a Template {#Usingatemplate}

The template represents a selection of variables to be collected and a format for transfer. You can select templates created and maintained by Azion or customize your own selection.

* **Custom Template**: 

  Choose the _Custom Template_ option to create your own customized _Data Set_, in JSON format, and select the variables that best suit your needs. 

> On the [Data Sources](#SelectingDataSources) list you'll find a description of all the available variables. Give it a try!

By default, Data Streaming will send your events when the block reaches 2000 records or every 60 seconds, whichever occurs first. But in case you choose the AWS Kinesis Firehose endpoint, Data Streaming will send your events when the block reaches 500 records or every 60 seconds.

Your records will be separated by the character `\n`, and sent in the payload to your endpoint.

---

## 4. Associating Domains {#Associatingdomains}

You can associate Data Streaming with one or more of your domains registered with Azion.

When associating a domain with Data Streaming, the events associated with that domain will be collected and sent to its endpoint.

You can also set the percentage of data you want to receive from your Data Streaming through the Sampling option. In addition to filtering by sampling, it can also reduce costs of data collection and analysis.

- **Sampling:**

1. Select the **All Domains** option to enable  Sampling.
2. Enter the number for the percentage of data you want to receive. This percentage will return the total data related to all your domains.

> When the Sampling option is enabled, you are allowed to add only one Data Streaming. If this Data Streaming is disabled, the Add Data Streaming option will be enabled again.

---

## 5. Setting the Endpoint {#Settingtheendpoint}

The Endpoint is the destination where you want to send the data collected by Azion. The endpoint type represents the method that your endpoint is configured to receive data from the Data Streaming. 

See how to configure the endpoints available in Real-Time Manager as follows.

### Apache Kafka {#ApacheKafka}

When setting up Data Streaming to send data to a Kafka endpoint in your infrastructure, you need first to obtain the following information to fill in the Real-Time Manager fields.

* **Bootstrap Servers:**  refers to the servers in the Kafka cluster, in this format *host1: port1, host2: port2, ...*, for example.

  You can add one or more Kafka servers to receive data collected by Data Streaming. When adding multiple servers, separate each *host:door* with a comma and no space, as in this example:

  *myownhost.com:2021,imaginaryhost.com:4525,anotherhost:4030* 

  This list should have just a few servers that will be used for the initial connection. There is no need to include all of the servers in your cluster.  

  > We recommend that you use more than one server to increase redundancy and availability.

* **Kafka Topic:**  refers to the Topic where Data Streaming should send messages in your cluster. This field only accepts a single Topic.

  In this field, you have to inform the Topic name where the messages will be posted. Even if multiple addresses have been registered in the *Bootstrap Servers* field, you can register only one destination topic. 

  Example: *azure.analytics.fct.pageviews.0*
  
  For more details on Apache Kafka settings, see [here](https://kafka.apache.org/quickstart).

#### Transport Layer Security (TLS)

You can use Data Streaming to send encrypted data using **Transport Layer Security (TLS)**, the successor to Secure Sockets Layer (SSL). 

TLS is a cryptographic protocol designed to provide communication security on a computer network. It is widely used in applications such as email, instant messaging, and voice over IP, but its use as an HTTPS security layer is still the most common.

If you want to use this protocol, make sure the endpoint that will receive the data is properly protected with a digital certificate issued by a globally recognized *Certificate Authority (CA)*, such as the ones below:

- IndenTrust
- DigiCert
- Sectigo
- GoDaddy
- GlobalSign
- Let’s Encrypt

> The TLS functionality is only available for Apache Kafka endpoints.

### AWS Kinesis Data Firehose {#AWS}

By using this type of endpoint, the Data Streaming service sends data directly to your AWS Kinesis Data Firehose service.

When using AWS Kinesis Data Firehose, your events will be grouped in blocks of up to 500 records, instead of the default 2000, or every 60 seconds, whichever occurs first.

- **Stream Name**: refers to the stream name that the user defined when they created the Kinesis Data Firehose. For example: _MyKDFConnector_

- **Region**: refers to the region in which we are going to send the data. For example: _us-east-1_.

- **Access Key**: refers to the public key to access the Data Firehose, which is given by AWS.

  For example: *ORIA5ZEH9MW4NL5OITY4*

- **Secret Key**: refers to the secret key to access the Data Firehose, which is given by AWS.

  For example: *+PLjkUWJyOLth3anuWXcLLVrMLeiiiThIokaPEiw*

### Data Dog {#DataDog}

By using this type of endpoint, the Data Streaming service sends data to a Data Dog endpoint in your infrastructure.

- **Data Dog URL**: (_requiered_) refers to the URI that will receive the collected data from Data Streaming.

- **API Key**: (_required_) refers to the key generated through the Data Dog panel and it is unique to your organization. An API Key is required by the Data Dog agent to submit metrics and events to Data Dog.

  For example: _ij9076f1ujik17a81f938yhru5g713422_

### Elasticsearch {#Elasticsearch}

By using this type of endpoint, the Data Streaming service sends data to a Elasticsearch endpoint in your infrastructure.

- **Elasticsearch URL**: (_required_) refers to the URL address plus the Elasticsearch index that will receive the data collected from Data Streaming. Here you must enter the data destination URL, followed by the index. 

  For example: *https://elasticsearch-domain.com/myindex*

- **API Key**: (_required_) refers to the base64 key, provided by Elasticsearch and will be used for validation with Elasticsearch. When generated, the key will look like this example: 

  *VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw*

### Google BigQuery {#Google}

By using this type of endpoint, the Data Streaming service sends data to a Google BigQuery endpoint in your infrastructure.

- **Project ID:** refers to your project ID on Google Cloud. This is a unique identifier and may have been generated by Google or customized by the customer.

  For example: *mycustomGBQproject01*

- **Dataset ID:** refers to your dataset ID on Google BigQuery, that is, the name that you have given to your dataset. It is a unique identifier per project and case sensitive.

  For example: *myGBQdataset*

- **Table ID:** refers to your table ID on Google BigQuery, that is, the name that you choose for the table. Just as the dataset must have a unique name per project, the table name also has to be unique per dataset. 

  For example: *mypagaviewtable01*

- **Service Account Key:** refers to the JSON file that is provided by Google Cloud. It has the following format:

``` json
{
    "type": "service_account",
    "project_id": "mycustomGBQproject01",
    "private_key_id": "key-id",
    "private_key": "-----BEGIN PRIVATE KEY-----\nprivate-key\n-----END PRIVATE KEY-----\n",
    "client_email": "service-account-email",
    "client_id": "client-id",
    "auth_uri": "https://accounts.google.com/o/oauth2/auth",
    "token_uri": "https://accounts.google.com/o/oauth2/token",
    "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
    "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/service-account-email"
}
```

### IBM QRadar {#IBM}

By using this type of endpoint, the Data Streaming service sends data in an HTTP POST payload to be  processed on your platform.

- **URL**: The URL that will receive the collected data from Data Streaming.

### S3 - Simple Storage Service {#S3}

By using this type of endpoint, the Data Streaming service sends data directly to any storage that works with the S3 protocol, such as Amazon S3, among others.

* **Host URL:** refers to the URL of the Host S3. You can connect with any provider that works with the S3 protocol.

  For example: *http://myownhost.s3.us-east-1.myprovider.com*

* **Bucket Name:**  refers to the name of the Bucket that the object will be sent to.

  It is important that the Bucket is already created to enable Data Streaming to send the objects. 

  You define the Bucket name. For example: *mys3bucket*

* **Region:**  refers to the region in which the Bucket is hosted. 

  For example:  *us-east-1*.

* **Access Key:**  refers to the public key to access the Bucket, which is given by your provider.

  For example: *ORIA5ZEH9MW4NL5OITY4*

* **Secret Key:**  refers to the secret key to access the Bucket, which is given by your provider.

  For example: *+PLjkUWJyOLth3anuWXcLLVrMLeiiiThIokaPEiw*

* **Object Key Prefix:**  refers to a prefix that you can add to the files that will be sent.
  The objects name are composed of *Prefix* + *Timestamp* + *UUID*. 

  For example:  if you use *waf_log*s as prefix, one of the sent objects will be saved like *waf_logs_1622575860091_37d66e78-c308-4006-9d4d-1c013ed89276*.

* **Content Type:**  refers to the format in which the object will be created in the Bucket. 

  You count on the *plain/text* and *application/gzip* options.


### Splunk {#Splunk}

By using this type of endpoint, the Data Streaming service sends data to a Splunk endpoint in your infrastructure.

- **Splunk URL**: (_required_) refers to the URL that will receive the collected data from Data Streaming. If you have an alternative index to point, you can do it at the end of the URL. 

  For example:*https://inputs.splunkcloud.com:8080/services/collector?index=myindex.*

- **API Key**: (_required_) refers to the HTTP Event Collector Token, provided by your Splunk installation. It will look like this example: *crfe25d2-23j8-48gf-a9ks-6b75w3ska674*

### Standard HTTP/HTTPS POST {#StandardHTTPHTTPSPOST}

By using this type of endpoint, the Data Streaming service sends data in an HTTP POST payload to be  processed on your platform.

* **Endpoint URL:** refers to the URL configured on the platform to receive Data Streaming data.

  Use the format *scheme://domain/path*. 

  For example: *https://app.domain.com/*

* **Custom Headers:**  you can enter one or more custom headers for your HTTP / HTTPS request. 

  For the headers configuration, you have to inform the *Name* and *Value* for each header. 

------

## 6. Customizing the Payload {#Customizingthepayload}

You can also customize the Payload sent by the connector according to the following options.

- **Log Line Separator**: (_optional_) defines the type of information that will be used at the end of each log line. 

- **Payload Format**: (_optional_) defines which information will be at the beginning and at the end of the dataset. 

For example, if the *Log Line Separator* receives "," and the *Payload Format* receives "[&dataset]", the user will get the following exit code:"

``` json
    [
    	{
			"request_method": "GET",
         	"host": "statis.exampledomain.com.br",
          "status": "200"
        },
        {
        	"request_method": "GET",
          	"host": "statis.exampledomain.com.br",
          	"status": "200"
        }
	]
```

> This feature is only available for the HTTP POST endpoint. 

------

## 7. Activating your Settings {#Activatingyoursettings}

You will find the following buttons at the bottom of the screen:

* **Active**: turn this button into active to enable your settings on the system. 

* **Cancel**: with this option, you return to the Data Streaming home page, also discard your edits. 

* **Save**: once your selections are complete, save your settings by clicking the **Save** button. 



---

Didn't find what you were looking for? [Open a support ticket.](https://tickets.azion.com/)
